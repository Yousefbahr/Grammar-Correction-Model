{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GnFB_gU9w96",
    "outputId": "e0dcfc3d-fa17-465d-8d4c-659bd9a90ead"
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SpIpVgi96jG"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BartModel, BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq, T5ForConditionalGeneration\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa4jZqKSYD0C"
   },
   "outputs": [],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0qOuWcCr5wM"
   },
   "source": [
    "**Freeze all layers except the last one which is reponsible for sequence generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAUSYev6Z2Ks",
    "outputId": "daa5d0cd-5878-44fc-e746-d13f38abde35"
   },
   "outputs": [],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PnC2j1sYjfg"
   },
   "outputs": [],
   "source": [
    "# freeze all layers except the last one which is used for seq generation\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "  param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQZmCv66altk",
    "outputId": "60b634eb-f3bd-463f-e611-c476dedd8296"
   },
   "outputs": [],
   "source": [
    "for name , param in model.named_parameters(remove_duplicate=False):\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5rmkluVsIN2"
   },
   "source": [
    "**Download the dataset and start preprocssing it**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bM4oh5tlEJTI",
    "outputId": "c5d43ffe-29ef-4ca8-f47d-ee7d5552cb2c"
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/liweili/c4_200m/resolve/main/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znCOC1pme85K",
    "outputId": "8d440d4a-6d90-45fa-e09d-341bd05babce"
   },
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67ApI0cquqRZ",
    "outputId": "0e02ca41-a5f8-483c-8603-052f72b55b60"
   },
   "outputs": [],
   "source": [
    "# checking number of lines\n",
    "\n",
    "!wc -l C4_200M.tsv-00000-of-00010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3iurx-6rFDa"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "with open(\"C4_200M.tsv-00000-of-00010\", \"r\") as f:\n",
    "\n",
    "  for i, line in enumerate(f):\n",
    "    if i == 500000:\n",
    "      break\n",
    "\n",
    "    x, y = line.split(\"\\t\")\n",
    "    X.append(\"fix grammar: \" + x)\n",
    "    Y.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56IcmpNt3XYE"
   },
   "outputs": [],
   "source": [
    "dataset  = pd.DataFrame({\"X\": X, \"Y\": Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNWykjcSCu30"
   },
   "source": [
    "The whole majority of the sequences' lengths lies between 0 and 500 words.\n",
    "\n",
    "This will matter when deciding how much to truncate and max length of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "Kogd-kC--dDV",
    "outputId": "75495c0b-3202-433e-a7bb-c6fe028dae5e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "X_lengths = dataset[\"X\"].apply(len)\n",
    "\n",
    "Y_lengths = dataset[\"Y\"].apply(len)\n",
    "\n",
    "# Get the frequency distribution of the lengths\n",
    "plt.hist(X_lengths.values)\n",
    "plt.hist(Y_lengths.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f4c987b89dcb420ab94cd607d3efd2b4",
      "5a95f65e4d884ddd9b1bcc90460fe480",
      "3ee395bea82f47f6b61feeee92b3cb90",
      "30ac35d6c63044e2a3e9f7046fb4e857",
      "dfb255f7d58c443ebdcf70b653a768e4",
      "8a0dcd369bcf4db5bded4c0630c14b3b",
      "faf0555300f943349170d61f6aaba5c7",
      "de1ebb49984a420a9c29ff817552ca21",
      "80916d2beb724b2c84fb73431a09915b",
      "b8c955f21e964ecd9c0a218ed8a6f48d",
      "dc784985619043bea409870842c3aa4e"
     ]
    },
    "id": "cIHNXT-c9vB9",
    "outputId": "4f577bfb-a321-4c56-949f-af45ea543f59"
   },
   "outputs": [],
   "source": [
    "# hugging face dataset for efficient tokenization\n",
    "hugging_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "def tokenize_function(data):\n",
    "  return tokenizer(data[\"X\"], text_target = data[\"Y\"], truncation=True, max_length=1024)\n",
    "\n",
    "\n",
    "tokenized_dataset = hugging_dataset.map(tokenize_function, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVGHxm1R9HLU",
    "outputId": "d9fb281b-1e5a-40c3-ffb7-a8e10e250369"
   },
   "outputs": [],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FZ4_TUU8ffb"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"X\", \"Y\"])\n",
    "tokenized_dataset.set_format(\"numpy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gU_5sMW5qro"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "val_dataset = tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmzisBGf8D1Q",
    "outputId": "b6864287-e9f2-4548-9f46-d627426e587d"
   },
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdBU_ChP8dyq"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_uHy1K34vln"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size = 8, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size = 8, collate_fn = data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQXZO_T-Kdnn",
    "outputId": "1be3527e-be13-4d51-c3e7-419b1d99d188"
   },
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#   break\n",
    "\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLmPrZvHYFlY"
   },
   "outputs": [],
   "source": [
    "# outputs = model(**batch)\n",
    "# print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6W0tAVNlGfu"
   },
   "outputs": [],
   "source": [
    "# distribute training acoss multiple GPUs\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23qrQU6QKnzB"
   },
   "outputs": [],
   "source": [
    "epochs_to_run = 1\n",
    "# number of batches to report loss\n",
    "batch_report_every = 300\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "metric = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fHroWNMnB_O"
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"/kaggle/working/checkpoint_epoch_0.pth\",  map_location=torch.device('cpu'))\n",
    "\n",
    "# model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "# starting_epoch = checkpoint[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in optim.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new learning rate\n",
    "# for param_group in optim.param_groups:\n",
    "#     param_group[\"lr\"] = 1e-7\n",
    "    # print(param_group[\"lr\"])\n",
    "    \n",
    "    # param_group[\"lr\"] = lr=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gleu(model, dataloader, metric ,tokenizer, sample_size = 300):\n",
    "    \"\"\"\n",
    "    evaluate the 'metric' on sentences generated by the 'model' \n",
    "    using inputs randomly sampled (of total size 'sample_size') from the dataset underlying the 'dataloader'.\n",
    "\n",
    "    return gleu score averaged across all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # sample indices without replacement(no duplicates)\n",
    "    indices = np.random.choice(range(len(dataloader.dataset)), size=sample_size, replace=False )\n",
    "    random_dataloader = torch.utils.data.DataLoader(dataloader.dataset, \n",
    "                                                    batch_size= dataloader.batch_size,\n",
    "                                                   sampler = torch.utils.data.SubsetRandomSampler(indices),\n",
    "                                                   collate_fn = data_collator)\n",
    "    scores = []\n",
    "    pad_index = tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "        for batch in random_dataloader:\n",
    "            \n",
    "            batch = {k : v.cuda() for k, v in batch.items()}\n",
    "            \n",
    "            # get max num of tokens(without padding) among sentences in the batch\n",
    "            # to determine the max new tokens when generating\n",
    "            max_new_tokens = max((batch[\"input_ids\"] != pad_index).sum(dim = -1))\n",
    "            # the number of tokens of the generated sentence should not differ vastly with its input counterpart\n",
    "            outputs = model.module.generate(**batch, \n",
    "                                       max_length= int(max_new_tokens + 10),\n",
    "                                       num_beams=4,\n",
    "                                        length_penalty=1.0)\n",
    "\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            batch[\"labels\"][batch[\"labels\"] == -100] = pad_index\n",
    "\n",
    "            references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "            # avg score across elements in a batch\n",
    "            score = metric.compute(predictions=preds, references=references)\n",
    "            \n",
    "            scores.append(score[\"google_bleu\"])\n",
    "\n",
    "    # avg score across batches\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, val_dataloader):\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "      batch = {k : v.cuda() for k, v in batch.items()}\n",
    "\n",
    "      outputs = model(**batch)\n",
    "\n",
    "      val_loss += outputs.loss.mean().item()\n",
    "\n",
    "  val_loss /= len(val_dataloader)\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used on gleu\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode='max', factor=0.3, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = starting_epoch + epochs_to_run\n",
    "model = model.cuda()\n",
    "\n",
    "for epoch in range(starting_epoch, Epochs):\n",
    "  print(f\"Epoch: {epoch}\")\n",
    "  model.train()\n",
    "  avg_batch_loss = 0\n",
    "  epoch_loss = 0\n",
    "    \n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    batch = {k : v.cuda() for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # loss.mean() because of distributed training\n",
    "    loss = outputs.loss.mean()\n",
    "      \n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "      \n",
    "    avg_batch_loss += loss.item()\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    # print avg batch loss\n",
    "    if not (i + 1) % batch_report_every:\n",
    "      avg_batch_loss /= batch_report_every\n",
    "      print(f\"-----Batches {i + 1 - batch_report_every} -- {i+1} | Avg Batch Training Loss: {avg_batch_loss}\", flush=True)\n",
    "      avg_batch_loss = 0\n",
    "\n",
    "    # save after 5,000 batches\n",
    "    if not (i + 1) % 5000:\n",
    "        print(f\"Saved Model at Batch {i}\", flush=True)\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "        }, f'checkpoint_epoch_{epoch}_step_{i}.pth')\n",
    " \n",
    "        gleu_score = evaluate_gleu(model, val_dataloader, metric, tokenizer)\n",
    "        val_loss = evaluate_loss(model, val_dataloader)\n",
    "        print(f\"-----Gleu Score On Validation Data: {gleu_score}\", flush=True)\n",
    "        print(f\"-----Val Loss: {val_loss}\", flush=True)\n",
    "        with open(\"gleu_scores.txt\", \"a\") as file:\n",
    "            file.write(f\"Epoch: {epoch} | Batch: {i} | Gleu Score: {gleu_score} | Val loss: {val_loss}\\n\")\n",
    "\n",
    "        scheduler.step(gleu_score)\n",
    "        \n",
    "        \n",
    "  epoch_loss /= len(train_dataloader)\n",
    "  print(f\"-----Training Loss: {epoch_loss}\", flush=True)\n",
    "    \n",
    "  # save after each epoch\n",
    "  torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "    }, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "  val_loss = evaluate_loss(model, val_dataloader)\n",
    "  \n",
    "  print(f\"-----Validation Loss: {val_loss}\", flush=True)\n",
    "  print(\"=============================================\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(model, metric, ungrammatical_sen, target =None):\n",
    "    model.eval()\n",
    "    google_bleu = None\n",
    "    \n",
    "    inputs = tokenizer(ungrammatical_sen, truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "\n",
    "    inputs = {k : v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.module.generate(**inputs,\n",
    "                                    max_length=len(ungrammatical_sen) + 20, \n",
    "                                    num_beams=5, \n",
    "                                    do_sample=True,\n",
    "                                   repetition_penalty=2.6,\n",
    "                                     temperature= 0.01\n",
    "                                    )\n",
    "\n",
    "    sentence = tokenizer.decode(outputs[0], skip_special_tokens=True )\n",
    "    \n",
    "    # compute gleu score between target and pred\n",
    "    if target:\n",
    "        google_bleu = metric.compute(predictions=[sentence], references=[target])\n",
    "        \n",
    "    return sentence, google_bleu\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"Bitcoin is for $7,094 this morning, which CoinDesk says.\",\n",
    "#                target = \"Bitcoin goes for $7,094 this morning, according to CoinDesk.\")\n",
    "\n",
    "inp = \"fix grammar:My names is ali and i went at school yesterday\"\n",
    "\n",
    "sentence, score = correct_grammar(model, \n",
    "                                  metric, \n",
    "                                  inp,\n",
    "                            target = \"My name is ali and i went to school yesterday\")\n",
    "\n",
    "print(f\"Input: {inp}\")\n",
    "print(f\"Output: {sentence}\")\n",
    "if score:\n",
    "    print(f\"Google Bleu: {score['google_bleu'] }\")\n",
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"The effect of widespread dud targets two face up attack position monsters on the field\",\n",
    "#                target = 'The effect of \"widespread dud\" targets two face up attack position monsters on the field.')\n",
    "               \n",
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"tax on sales of stores for non residents are set at 21% for 2014 and 20% in 2015 payable on sales tentatively earned from the difference of the property value some time of purchase (price differences according to working time) and theyear to which sale couples (sales costs), based on the approved annual on the base approved by law).\",\n",
    "#                target = \"Capital Gains tax on the sale of properties for non-residents is set at 21% for 2014 and 20% in 2015 payable on profits earned on the difference of the property value between the year of purchase (purchase price plus costs) and the year of sale (sales price minus costs), based on the approved annual percentage increase on the base value approved by law.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31013,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
