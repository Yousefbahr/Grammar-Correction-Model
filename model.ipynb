{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BartModel, BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq, T5ForConditionalGeneration\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0qOuWcCr5wM"
   },
   "source": [
    "**Freeze all layers except the last one which is reponsible for sequence generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers except the last one which is used for seq generation\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "  param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight True\n",
      "encoder.embed_tokens.weight True\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
      "encoder.block.0.layer.0.layer_norm.weight False\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.0.layer.1.layer_norm.weight False\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.1.layer.0.layer_norm.weight False\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.1.layer.1.layer_norm.weight False\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.2.layer.0.layer_norm.weight False\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.2.layer.1.layer_norm.weight False\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.3.layer.0.layer_norm.weight False\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.3.layer.1.layer_norm.weight False\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.4.layer.0.layer_norm.weight False\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.4.layer.1.layer_norm.weight False\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.5.layer.0.layer_norm.weight False\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.5.layer.1.layer_norm.weight False\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.6.layer.0.layer_norm.weight False\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.6.layer.1.layer_norm.weight False\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.7.layer.0.layer_norm.weight False\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.7.layer.1.layer_norm.weight False\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.8.layer.0.layer_norm.weight False\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.8.layer.1.layer_norm.weight False\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.9.layer.0.layer_norm.weight False\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.9.layer.1.layer_norm.weight False\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.10.layer.0.layer_norm.weight False\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.10.layer.1.layer_norm.weight False\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight False\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight False\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight False\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight False\n",
      "encoder.block.11.layer.0.layer_norm.weight False\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.weight False\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight False\n",
      "encoder.block.11.layer.1.layer_norm.weight False\n",
      "encoder.final_layer_norm.weight False\n",
      "decoder.embed_tokens.weight True\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
      "decoder.block.0.layer.0.layer_norm.weight False\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.0.layer.1.layer_norm.weight False\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.0.layer.2.layer_norm.weight False\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.1.layer.0.layer_norm.weight False\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.1.layer.1.layer_norm.weight False\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.1.layer.2.layer_norm.weight False\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.2.layer.0.layer_norm.weight False\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.2.layer.1.layer_norm.weight False\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.2.layer.2.layer_norm.weight False\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.3.layer.0.layer_norm.weight False\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.3.layer.1.layer_norm.weight False\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.3.layer.2.layer_norm.weight False\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.4.layer.0.layer_norm.weight False\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.4.layer.1.layer_norm.weight False\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.4.layer.2.layer_norm.weight False\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.5.layer.0.layer_norm.weight False\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.5.layer.1.layer_norm.weight False\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.5.layer.2.layer_norm.weight False\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.6.layer.0.layer_norm.weight False\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.6.layer.1.layer_norm.weight False\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.6.layer.2.layer_norm.weight False\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.7.layer.0.layer_norm.weight False\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.7.layer.1.layer_norm.weight False\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.7.layer.2.layer_norm.weight False\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.8.layer.0.layer_norm.weight False\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.8.layer.1.layer_norm.weight False\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.8.layer.2.layer_norm.weight False\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.9.layer.0.layer_norm.weight False\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.9.layer.1.layer_norm.weight False\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.9.layer.2.layer_norm.weight False\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.10.layer.0.layer_norm.weight False\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.10.layer.1.layer_norm.weight False\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.10.layer.2.layer_norm.weight False\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight False\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight False\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight False\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight False\n",
      "decoder.block.11.layer.0.layer_norm.weight False\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight False\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight False\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight False\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight False\n",
      "decoder.block.11.layer.1.layer_norm.weight False\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.weight False\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight False\n",
      "decoder.block.11.layer.2.layer_norm.weight False\n",
      "decoder.final_layer_norm.weight False\n",
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters(remove_duplicate=False):\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5rmkluVsIN2"
   },
   "source": [
    "**Download the dataset and start preprocssing it**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/liweili/c4_200m/resolve/main/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of lines\n",
    "\n",
    "!wc -l C4_200M.tsv-00000-of-00010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "with open(\"C4_200M.tsv-00000-of-00010\", \"r\") as f:\n",
    "\n",
    "  for i, line in enumerate(f):\n",
    "    if i == 500000:\n",
    "      break\n",
    "\n",
    "    x, y = line.split(\"\\t\")\n",
    "    X.append(\"fix grammar: \" + x)\n",
    "    Y.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = pd.DataFrame({\"X\": X, \"Y\": Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNWykjcSCu30"
   },
   "source": [
    "The whole majority of the sequences' lengths lies between 0 and 500 words.\n",
    "\n",
    "This will matter when deciding how much to truncate and max length of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fix grammar: Bitcoin is for $7,094 this mornin...</td>\n",
       "      <td>Bitcoin goes for $7,094 this morning, accordin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fix grammar: The effect of widespread dud targ...</td>\n",
       "      <td>1. The effect of \"widespread dud\" targets two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fix grammar: tax on sales of stores for non re...</td>\n",
       "      <td>Capital Gains tax on the sale of properties fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fix grammar: Much many brands and sellers stil...</td>\n",
       "      <td>Many brands and sellers still in the market.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fix grammar: this is is the latest Maintenance...</td>\n",
       "      <td>This is is the latest maintenance release of S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fix grammar: Fairy Or Not, I'm the Godmother: ...</td>\n",
       "      <td>Fairy Or Not, I'm the Godmother: Not just a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fix grammar: Watcch as this Dodge Challenger H...</td>\n",
       "      <td>Watch as this Dodge Challenger Hellcat gets sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fix grammar: Momover, these devices have been ...</td>\n",
       "      <td>Moreover, these devices are proven to help con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fix grammar: Ever cloud has a silver lining an...</td>\n",
       "      <td>Every cloud has a silver lining and it’s just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fix grammar: Worthless involved's supporting f...</td>\n",
       "      <td>Get involved and help the movement!\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0  fix grammar: Bitcoin is for $7,094 this mornin...   \n",
       "1  fix grammar: The effect of widespread dud targ...   \n",
       "2  fix grammar: tax on sales of stores for non re...   \n",
       "3  fix grammar: Much many brands and sellers stil...   \n",
       "4  fix grammar: this is is the latest Maintenance...   \n",
       "5  fix grammar: Fairy Or Not, I'm the Godmother: ...   \n",
       "6  fix grammar: Watcch as this Dodge Challenger H...   \n",
       "7  fix grammar: Momover, these devices have been ...   \n",
       "8  fix grammar: Ever cloud has a silver lining an...   \n",
       "9  fix grammar: Worthless involved's supporting f...   \n",
       "\n",
       "                                                   Y  \n",
       "0  Bitcoin goes for $7,094 this morning, accordin...  \n",
       "1  1. The effect of \"widespread dud\" targets two ...  \n",
       "2  Capital Gains tax on the sale of properties fo...  \n",
       "3     Many brands and sellers still in the market.\\n  \n",
       "4  This is is the latest maintenance release of S...  \n",
       "5  Fairy Or Not, I'm the Godmother: Not just a lo...  \n",
       "6  Watch as this Dodge Challenger Hellcat gets sm...  \n",
       "7  Moreover, these devices are proven to help con...  \n",
       "8  Every cloud has a silver lining and it’s just ...  \n",
       "9              Get involved and help the movement!\\n  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.17901e+05, 1.37989e+05, 3.21820e+04, 7.98600e+03, 2.52800e+03,\n",
       "        8.82000e+02, 3.99000e+02, 1.14000e+02, 1.20000e+01, 7.00000e+00]),\n",
       " array([  13. ,  139.3,  265.6,  391.9,  518.2,  644.5,  770.8,  897.1,\n",
       "        1023.4, 1149.7, 1276. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "X_lengths = dataset[\"X\"].apply(len)\n",
    "\n",
    "Y_lengths = dataset[\"Y\"].apply(len)\n",
    "\n",
    "# Get the frequency distribution of the lengths\n",
    "plt.hist(X_lengths.values)\n",
    "plt.hist(Y_lengths.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fcd0477f9d450db17962c29033b1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hugging face dataset for efficient tokenization\n",
    "hugging_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "def tokenize_function(data):\n",
    "  return tokenizer(data[\"X\"], text_target = data[\"Y\"], truncation=True, max_length=1024)\n",
    "\n",
    "\n",
    "tokenized_dataset = hugging_dataset.map(tokenize_function, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['X', 'Y', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 500000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"X\", \"Y\"])\n",
    "tokenized_dataset.set_format(\"numpy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "val_dataset = tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 450000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size = 8, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size = 8, collate_fn = data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#   break\n",
    "\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(**batch)\n",
    "# print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute training acoss multiple GPUs\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c37495776754d20a8c33f54392e4d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105b3299ef84163baa9da71fda9059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_to_run = 1\n",
    "# number of batches to report loss\n",
    "batch_report_every = 300\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "metric = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"/kaggle/working/checkpoint_epoch_0.pth\",  map_location=torch.device('cpu'))\n",
    "\n",
    "# model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "# starting_epoch = checkpoint[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in optim.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new learning rate\n",
    "# for param_group in optim.param_groups:\n",
    "#     param_group[\"lr\"] = 1e-7\n",
    "    # print(param_group[\"lr\"])\n",
    "    \n",
    "    # param_group[\"lr\"] = lr=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gleu(model, dataloader, metric ,tokenizer, sample_size = 300):\n",
    "    \"\"\"\n",
    "    evaluate the 'metric' on sentences generated by the 'model' \n",
    "    using inputs randomly sampled (of total size 'sample_size') from the dataset underlying the 'dataloader'.\n",
    "\n",
    "    return gleu score averaged across all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # sample indices without replacement(no duplicates)\n",
    "    indices = np.random.choice(range(len(dataloader.dataset)), size=sample_size, replace=False )\n",
    "    random_dataloader = torch.utils.data.DataLoader(dataloader.dataset, \n",
    "                                                    batch_size= dataloader.batch_size,\n",
    "                                                   sampler = torch.utils.data.SubsetRandomSampler(indices),\n",
    "                                                   collate_fn = data_collator)\n",
    "    scores = []\n",
    "    pad_index = tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "        for batch in random_dataloader:\n",
    "            \n",
    "            batch = {k : v.cuda() for k, v in batch.items()}\n",
    "            \n",
    "            # get max num of tokens(without padding) among sentences in the batch\n",
    "            # to determine the max new tokens when generating\n",
    "            max_new_tokens = max((batch[\"input_ids\"] != pad_index).sum(dim = -1))\n",
    "            # the number of tokens of the generated sentence should not differ vastly with its input counterpart\n",
    "            outputs = model.module.generate(**batch, \n",
    "                                       max_length= int(max_new_tokens + 10),\n",
    "                                       num_beams=4,\n",
    "                                        length_penalty=1.0)\n",
    "\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            batch[\"labels\"][batch[\"labels\"] == -100] = pad_index\n",
    "\n",
    "            references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "            # avg score across elements in a batch\n",
    "            score = metric.compute(predictions=preds, references=references)\n",
    "            \n",
    "            scores.append(score[\"google_bleu\"])\n",
    "\n",
    "    # avg score across batches\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, val_dataloader):\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "      batch = {k : v.cuda() for k, v in batch.items()}\n",
    "\n",
    "      outputs = model(**batch)\n",
    "\n",
    "      val_loss += outputs.loss.mean().item()\n",
    "\n",
    "  val_loss /= len(val_dataloader)\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used on gleu\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode='max', factor=0.3, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "-----Batches 0 -- 300 | Avg Batch Training Loss: 1.2702706710497538\n",
      "-----Batches 300 -- 600 | Avg Batch Training Loss: 1.3052358520030976\n",
      "-----Batches 600 -- 900 | Avg Batch Training Loss: 1.308895122806231\n",
      "-----Batches 900 -- 1200 | Avg Batch Training Loss: 1.2768433554967245\n",
      "-----Batches 1200 -- 1500 | Avg Batch Training Loss: 1.2581043761968613\n",
      "-----Batches 1500 -- 1800 | Avg Batch Training Loss: 1.2324114471673966\n",
      "-----Batches 1800 -- 2100 | Avg Batch Training Loss: 1.244865139524142\n",
      "-----Batches 2100 -- 2400 | Avg Batch Training Loss: 1.2197962548335393\n",
      "-----Batches 2400 -- 2700 | Avg Batch Training Loss: 1.2721824103593826\n",
      "-----Batches 2700 -- 3000 | Avg Batch Training Loss: 1.289998206694921\n",
      "-----Batches 3000 -- 3300 | Avg Batch Training Loss: 1.2372516387701034\n",
      "-----Batches 3300 -- 3600 | Avg Batch Training Loss: 1.2258735315004985\n",
      "-----Batches 3600 -- 3900 | Avg Batch Training Loss: 1.2207508985201518\n",
      "-----Batches 3900 -- 4200 | Avg Batch Training Loss: 1.300179082751274\n",
      "-----Batches 4200 -- 4500 | Avg Batch Training Loss: 1.2283125197887421\n",
      "-----Batches 4500 -- 4800 | Avg Batch Training Loss: 1.2527791941165924\n",
      "Saved Model at Batch 4999\n",
      "-----Gleu Score On Validation Data: 0.39028005344926325\n",
      "-----Val Loss: 1.1642256222438812\n",
      "-----Batches 4800 -- 5100 | Avg Batch Training Loss: 1.1914217799901963\n",
      "-----Batches 5100 -- 5400 | Avg Batch Training Loss: 1.1124266437689463\n",
      "-----Batches 5400 -- 5700 | Avg Batch Training Loss: 1.1854285117983818\n",
      "-----Batches 5700 -- 6000 | Avg Batch Training Loss: 1.2249245453874271\n",
      "-----Batches 6000 -- 6300 | Avg Batch Training Loss: 1.1527622052033741\n",
      "-----Batches 6300 -- 6600 | Avg Batch Training Loss: 1.1606534081697464\n",
      "-----Batches 6600 -- 6900 | Avg Batch Training Loss: 1.155224124789238\n",
      "-----Batches 6900 -- 7200 | Avg Batch Training Loss: 1.1400359326601028\n",
      "-----Batches 7200 -- 7500 | Avg Batch Training Loss: 1.1307883313298226\n",
      "-----Batches 7500 -- 7800 | Avg Batch Training Loss: 1.1443690647681555\n",
      "-----Batches 7800 -- 8100 | Avg Batch Training Loss: 1.115093582868576\n",
      "-----Batches 8100 -- 8400 | Avg Batch Training Loss: 1.0756519741813342\n",
      "-----Batches 8400 -- 8700 | Avg Batch Training Loss: 1.1119492016235988\n",
      "-----Batches 8700 -- 9000 | Avg Batch Training Loss: 1.117674794793129\n",
      "-----Batches 9000 -- 9300 | Avg Batch Training Loss: 1.1396648774544398\n",
      "-----Batches 9300 -- 9600 | Avg Batch Training Loss: 1.1129849215348562\n",
      "-----Batches 9600 -- 9900 | Avg Batch Training Loss: 1.1346695935726165\n",
      "Saved Model at Batch 9999\n",
      "-----Gleu Score On Validation Data: 0.438348323718013\n",
      "-----Val Loss: 1.1328688369607924\n",
      "-----Batches 9900 -- 10200 | Avg Batch Training Loss: 1.0826645280917486\n",
      "-----Batches 10200 -- 10500 | Avg Batch Training Loss: 1.1352444537480673\n",
      "-----Batches 10500 -- 10800 | Avg Batch Training Loss: 1.127619712948799\n",
      "-----Batches 10800 -- 11100 | Avg Batch Training Loss: 1.1053619958957037\n",
      "-----Batches 11100 -- 11400 | Avg Batch Training Loss: 1.1599871464570364\n",
      "-----Batches 11400 -- 11700 | Avg Batch Training Loss: 1.1213154383500417\n",
      "-----Batches 11700 -- 12000 | Avg Batch Training Loss: 1.1332751020789147\n",
      "-----Batches 12000 -- 12300 | Avg Batch Training Loss: 1.1380555218458175\n",
      "-----Batches 12300 -- 12600 | Avg Batch Training Loss: 1.120672988295555\n",
      "-----Batches 12600 -- 12900 | Avg Batch Training Loss: 1.129757538040479\n",
      "-----Batches 12900 -- 13200 | Avg Batch Training Loss: 1.0823856402436893\n",
      "-----Batches 13200 -- 13500 | Avg Batch Training Loss: 1.0824785459041595\n",
      "-----Batches 13500 -- 13800 | Avg Batch Training Loss: 1.1202827914555868\n",
      "-----Batches 13800 -- 14100 | Avg Batch Training Loss: 1.1621048665046692\n",
      "-----Batches 14100 -- 14400 | Avg Batch Training Loss: 1.0668994069099427\n",
      "-----Batches 14400 -- 14700 | Avg Batch Training Loss: 1.1058926542599996\n",
      "-----Batches 14700 -- 15000 | Avg Batch Training Loss: 1.1064184288183847\n",
      "Saved Model at Batch 14999\n",
      "-----Gleu Score On Validation Data: 0.45240555130093607\n",
      "-----Val Loss: 1.1093516298818589\n",
      "-----Batches 15000 -- 15300 | Avg Batch Training Loss: 1.0996864557266235\n",
      "-----Batches 15300 -- 15600 | Avg Batch Training Loss: 1.1118301107486088\n",
      "-----Batches 15600 -- 15900 | Avg Batch Training Loss: 1.0735675438245138\n",
      "-----Batches 15900 -- 16200 | Avg Batch Training Loss: 1.073449551065763\n",
      "-----Batches 16200 -- 16500 | Avg Batch Training Loss: 1.1227236592769623\n",
      "-----Batches 16500 -- 16800 | Avg Batch Training Loss: 1.1574300931890806\n",
      "-----Batches 16800 -- 17100 | Avg Batch Training Loss: 1.1437608356277147\n",
      "-----Batches 17100 -- 17400 | Avg Batch Training Loss: 1.0930078556140264\n",
      "-----Batches 17400 -- 17700 | Avg Batch Training Loss: 1.0840746945142745\n",
      "-----Batches 17700 -- 18000 | Avg Batch Training Loss: 1.129026263753573\n",
      "-----Batches 18000 -- 18300 | Avg Batch Training Loss: 1.1097233071923256\n",
      "-----Batches 18300 -- 18600 | Avg Batch Training Loss: 1.098238064646721\n",
      "-----Batches 18600 -- 18900 | Avg Batch Training Loss: 1.0043481654922168\n",
      "-----Batches 18900 -- 19200 | Avg Batch Training Loss: 1.082138362924258\n",
      "-----Batches 19200 -- 19500 | Avg Batch Training Loss: 1.1098790574073791\n",
      "-----Batches 19500 -- 19800 | Avg Batch Training Loss: 1.1036652255058288\n",
      "Saved Model at Batch 19999\n",
      "-----Gleu Score On Validation Data: 0.4829014543173157\n",
      "-----Val Loss: 1.0911445416259766\n",
      "-----Batches 19800 -- 20100 | Avg Batch Training Loss: 1.102850505510966\n",
      "-----Batches 20100 -- 20400 | Avg Batch Training Loss: 1.0532421163717907\n",
      "-----Batches 20400 -- 20700 | Avg Batch Training Loss: 1.1143083318074545\n",
      "-----Batches 20700 -- 21000 | Avg Batch Training Loss: 1.083979605436325\n",
      "-----Batches 21000 -- 21300 | Avg Batch Training Loss: 1.1027750932176907\n",
      "-----Batches 21300 -- 21600 | Avg Batch Training Loss: 1.1090342969695728\n",
      "-----Batches 21600 -- 21900 | Avg Batch Training Loss: 1.1004134851694107\n",
      "-----Batches 21900 -- 22200 | Avg Batch Training Loss: 1.109751873612404\n",
      "-----Batches 22200 -- 22500 | Avg Batch Training Loss: 1.1179298210144042\n",
      "-----Batches 22500 -- 22800 | Avg Batch Training Loss: 1.0559053933620453\n",
      "-----Batches 22800 -- 23100 | Avg Batch Training Loss: 1.0501906446615854\n",
      "-----Batches 23100 -- 23400 | Avg Batch Training Loss: 1.1042343082030615\n",
      "-----Batches 23400 -- 23700 | Avg Batch Training Loss: 1.1160877754290899\n",
      "-----Batches 23700 -- 24000 | Avg Batch Training Loss: 1.0642241406440736\n",
      "-----Batches 24000 -- 24300 | Avg Batch Training Loss: 1.1180600120623907\n",
      "-----Batches 24300 -- 24600 | Avg Batch Training Loss: 1.132901490132014\n",
      "-----Batches 24600 -- 24900 | Avg Batch Training Loss: 1.0635417188207308\n",
      "Saved Model at Batch 24999\n",
      "-----Gleu Score On Validation Data: 0.43667307950399487\n",
      "-----Val Loss: 1.0761404789352418\n",
      "-----Batches 24900 -- 25200 | Avg Batch Training Loss: 1.0725835913419723\n",
      "-----Batches 25200 -- 25500 | Avg Batch Training Loss: 1.0352082126339277\n",
      "-----Batches 25500 -- 25800 | Avg Batch Training Loss: 1.0658941304683685\n",
      "-----Batches 25800 -- 26100 | Avg Batch Training Loss: 1.0957949262857438\n",
      "-----Batches 26100 -- 26400 | Avg Batch Training Loss: 1.0830775907635688\n",
      "-----Batches 26400 -- 26700 | Avg Batch Training Loss: 1.1125455671548843\n",
      "-----Batches 26700 -- 27000 | Avg Batch Training Loss: 1.0651625102758409\n",
      "-----Batches 27000 -- 27300 | Avg Batch Training Loss: 1.0222384705146155\n",
      "-----Batches 27300 -- 27600 | Avg Batch Training Loss: 1.0696567972501119\n",
      "-----Batches 27600 -- 27900 | Avg Batch Training Loss: 1.0509791645407676\n",
      "-----Batches 27900 -- 28200 | Avg Batch Training Loss: 1.0638267157475154\n",
      "-----Batches 28200 -- 28500 | Avg Batch Training Loss: 1.0571579372882842\n",
      "-----Batches 28500 -- 28800 | Avg Batch Training Loss: 1.0542535547415415\n",
      "-----Batches 28800 -- 29100 | Avg Batch Training Loss: 1.0474744374553362\n",
      "-----Batches 29100 -- 29400 | Avg Batch Training Loss: 1.0048051708936692\n",
      "-----Batches 29400 -- 29700 | Avg Batch Training Loss: 1.0999342424670855\n",
      "-----Batches 29700 -- 30000 | Avg Batch Training Loss: 1.059601058959961\n",
      "Saved Model at Batch 29999\n",
      "-----Gleu Score On Validation Data: 0.4361061365445637\n",
      "-----Val Loss: 1.0640098794460298\n",
      "-----Batches 30000 -- 30300 | Avg Batch Training Loss: 0.9906774984796842\n",
      "-----Batches 30300 -- 30600 | Avg Batch Training Loss: 1.0885279326637587\n",
      "-----Batches 30600 -- 30900 | Avg Batch Training Loss: 1.0316934597492218\n",
      "-----Batches 30900 -- 31200 | Avg Batch Training Loss: 1.03281829059124\n",
      "-----Batches 31200 -- 31500 | Avg Batch Training Loss: 1.0464281155665716\n",
      "-----Batches 31500 -- 31800 | Avg Batch Training Loss: 1.0918028938770294\n",
      "-----Batches 31800 -- 32100 | Avg Batch Training Loss: 1.0768842845161757\n",
      "-----Batches 32100 -- 32400 | Avg Batch Training Loss: 1.0441749711831412\n",
      "-----Batches 32400 -- 32700 | Avg Batch Training Loss: 1.0622216755151748\n",
      "-----Batches 32700 -- 33000 | Avg Batch Training Loss: 1.0424480535586675\n",
      "-----Batches 33000 -- 33300 | Avg Batch Training Loss: 1.0648310764630635\n",
      "-----Batches 33300 -- 33600 | Avg Batch Training Loss: 1.0131029508511225\n",
      "-----Batches 33600 -- 33900 | Avg Batch Training Loss: 1.0827948850393296\n",
      "-----Batches 33900 -- 34200 | Avg Batch Training Loss: 0.9885461140672366\n",
      "-----Batches 34200 -- 34500 | Avg Batch Training Loss: 1.065413361589114\n",
      "-----Batches 34500 -- 34800 | Avg Batch Training Loss: 0.9967872340480487\n",
      "Saved Model at Batch 34999\n",
      "-----Gleu Score On Validation Data: 0.4901646569279507\n",
      "-----Val Loss: 1.0537869661855697\n",
      "-----Batches 34800 -- 35100 | Avg Batch Training Loss: 1.0390979860226313\n",
      "-----Batches 35100 -- 35400 | Avg Batch Training Loss: 1.0228325548768042\n",
      "-----Batches 35400 -- 35700 | Avg Batch Training Loss: 1.0422767145435015\n",
      "-----Batches 35700 -- 36000 | Avg Batch Training Loss: 1.1324494286378224\n",
      "-----Batches 36000 -- 36300 | Avg Batch Training Loss: 0.9992096390326818\n",
      "-----Batches 36300 -- 36600 | Avg Batch Training Loss: 1.0512242066860198\n",
      "-----Batches 36600 -- 36900 | Avg Batch Training Loss: 1.0205598851044972\n",
      "-----Batches 36900 -- 37200 | Avg Batch Training Loss: 1.0063473306099573\n",
      "-----Batches 37200 -- 37500 | Avg Batch Training Loss: 1.0134417275587717\n",
      "-----Batches 37500 -- 37800 | Avg Batch Training Loss: 0.977875488003095\n",
      "-----Batches 37800 -- 38100 | Avg Batch Training Loss: 1.0723571180303892\n",
      "-----Batches 38100 -- 38400 | Avg Batch Training Loss: 1.0432359858353932\n",
      "-----Batches 38400 -- 38700 | Avg Batch Training Loss: 1.0342175924777985\n",
      "-----Batches 38700 -- 39000 | Avg Batch Training Loss: 1.0677687728404999\n",
      "-----Batches 39000 -- 39300 | Avg Batch Training Loss: 1.054410212635994\n",
      "-----Batches 39300 -- 39600 | Avg Batch Training Loss: 0.9915650933980942\n",
      "-----Batches 39600 -- 39900 | Avg Batch Training Loss: 1.0028436479965845\n",
      "Saved Model at Batch 39999\n",
      "-----Gleu Score On Validation Data: 0.5272375543249934\n",
      "-----Val Loss: 1.0447866362953186\n",
      "-----Batches 39900 -- 40200 | Avg Batch Training Loss: 1.0696410003304482\n",
      "-----Batches 40200 -- 40500 | Avg Batch Training Loss: 1.027449040611585\n",
      "-----Batches 40500 -- 40800 | Avg Batch Training Loss: 1.059484608968099\n",
      "-----Batches 40800 -- 41100 | Avg Batch Training Loss: 1.0711742440859477\n",
      "-----Batches 41100 -- 41400 | Avg Batch Training Loss: 1.0584503351648649\n",
      "-----Batches 41400 -- 41700 | Avg Batch Training Loss: 1.0609967274467151\n",
      "-----Batches 41700 -- 42000 | Avg Batch Training Loss: 1.0471004213889439\n",
      "-----Batches 42000 -- 42300 | Avg Batch Training Loss: 1.0436787382761636\n",
      "-----Batches 42300 -- 42600 | Avg Batch Training Loss: 1.0640896418690682\n",
      "-----Batches 42600 -- 42900 | Avg Batch Training Loss: 1.0675141922632854\n",
      "-----Batches 42900 -- 43200 | Avg Batch Training Loss: 1.0879539495706558\n",
      "-----Batches 43200 -- 43500 | Avg Batch Training Loss: 1.0593208636840183\n",
      "-----Batches 43500 -- 43800 | Avg Batch Training Loss: 1.0000873480240504\n",
      "-----Batches 43800 -- 44100 | Avg Batch Training Loss: 1.0898701318105062\n",
      "-----Batches 44100 -- 44400 | Avg Batch Training Loss: 1.0320299748579662\n",
      "-----Batches 44400 -- 44700 | Avg Batch Training Loss: 1.0677213184038798\n",
      "-----Batches 44700 -- 45000 | Avg Batch Training Loss: 1.02521846195062\n",
      "Saved Model at Batch 44999\n",
      "-----Gleu Score On Validation Data: 0.5018059233859821\n",
      "-----Val Loss: 1.0373222772550583\n",
      "-----Batches 45000 -- 45300 | Avg Batch Training Loss: 1.0403919572631517\n",
      "-----Batches 45300 -- 45600 | Avg Batch Training Loss: 1.0137936833500862\n",
      "-----Batches 45600 -- 45900 | Avg Batch Training Loss: 1.0086521027485529\n",
      "-----Batches 45900 -- 46200 | Avg Batch Training Loss: 1.0219707813858987\n",
      "-----Batches 46200 -- 46500 | Avg Batch Training Loss: 1.0507375166813533\n",
      "-----Batches 46500 -- 46800 | Avg Batch Training Loss: 1.0642998566230137\n",
      "-----Batches 46800 -- 47100 | Avg Batch Training Loss: 0.9856771975755692\n",
      "-----Batches 47100 -- 47400 | Avg Batch Training Loss: 0.9990251396099726\n",
      "-----Batches 47400 -- 47700 | Avg Batch Training Loss: 1.0012850815057754\n",
      "-----Batches 47700 -- 48000 | Avg Batch Training Loss: 1.0010337222615877\n",
      "-----Batches 48000 -- 48300 | Avg Batch Training Loss: 0.9870507425069809\n",
      "-----Batches 48300 -- 48600 | Avg Batch Training Loss: 1.0069773737589518\n",
      "-----Batches 48600 -- 48900 | Avg Batch Training Loss: 1.0722321685155232\n",
      "-----Batches 48900 -- 49200 | Avg Batch Training Loss: 1.030469538072745\n",
      "-----Batches 49200 -- 49500 | Avg Batch Training Loss: 1.0390198932091395\n",
      "-----Batches 49500 -- 49800 | Avg Batch Training Loss: 0.9684266070524852\n",
      "Saved Model at Batch 49999\n",
      "-----Gleu Score On Validation Data: 0.5213801632500906\n",
      "-----Val Loss: 1.030379100213051\n",
      "-----Batches 49800 -- 50100 | Avg Batch Training Loss: 0.9712272527813911\n",
      "-----Batches 50100 -- 50400 | Avg Batch Training Loss: 1.055712023874124\n",
      "-----Batches 50400 -- 50700 | Avg Batch Training Loss: 0.9978893946607907\n",
      "-----Batches 50700 -- 51000 | Avg Batch Training Loss: 1.0056938655177752\n",
      "-----Batches 51000 -- 51300 | Avg Batch Training Loss: 0.9895643822352092\n",
      "-----Batches 51300 -- 51600 | Avg Batch Training Loss: 1.0462027500073114\n",
      "-----Batches 51600 -- 51900 | Avg Batch Training Loss: 1.0630826487143834\n",
      "-----Batches 51900 -- 52200 | Avg Batch Training Loss: 1.0494271598259608\n",
      "-----Batches 52200 -- 52500 | Avg Batch Training Loss: 0.9854424862066905\n",
      "-----Batches 52500 -- 52800 | Avg Batch Training Loss: 0.9718210115035375\n",
      "-----Batches 52800 -- 53100 | Avg Batch Training Loss: 1.0393621444702148\n",
      "-----Batches 53100 -- 53400 | Avg Batch Training Loss: 1.0036407206455866\n",
      "-----Batches 53400 -- 53700 | Avg Batch Training Loss: 1.0752231652537982\n",
      "-----Batches 53700 -- 54000 | Avg Batch Training Loss: 1.009275999267896\n",
      "-----Batches 54000 -- 54300 | Avg Batch Training Loss: 1.0078150329987208\n",
      "-----Batches 54300 -- 54600 | Avg Batch Training Loss: 1.025299681723118\n",
      "-----Batches 54600 -- 54900 | Avg Batch Training Loss: 0.9763077465693156\n",
      "Saved Model at Batch 54999\n",
      "-----Gleu Score On Validation Data: 0.4858866584901539\n",
      "-----Val Loss: 1.0234966056871415\n",
      "-----Batches 54900 -- 55200 | Avg Batch Training Loss: 1.0618336726228397\n",
      "-----Batches 55200 -- 55500 | Avg Batch Training Loss: 1.023802666068077\n",
      "-----Batches 55500 -- 55800 | Avg Batch Training Loss: 0.9494126503666241\n",
      "-----Batches 55800 -- 56100 | Avg Batch Training Loss: 1.0125181347131729\n",
      "-----Training Loss: 1.083293448109097\n",
      "-----Validation Loss: 1.023029755101204\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "Epochs = starting_epoch + epochs_to_run\n",
    "model = model.cuda()\n",
    "\n",
    "for epoch in range(starting_epoch, Epochs):\n",
    "  print(f\"Epoch: {epoch}\")\n",
    "  model.train()\n",
    "  avg_batch_loss = 0\n",
    "  epoch_loss = 0\n",
    "    \n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    batch = {k : v.cuda() for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # loss.mean() because of distributed training\n",
    "    loss = outputs.loss.mean()\n",
    "      \n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "      \n",
    "    avg_batch_loss += loss.item()\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    # print avg batch loss\n",
    "    if not (i + 1) % batch_report_every:\n",
    "      avg_batch_loss /= batch_report_every\n",
    "      print(f\"-----Batches {i + 1 - batch_report_every} -- {i+1} | Avg Batch Training Loss: {avg_batch_loss}\", flush=True)\n",
    "      avg_batch_loss = 0\n",
    "\n",
    "    # save after 5,000 batches\n",
    "    if not (i + 1) % 5000:\n",
    "        print(f\"Saved Model at Batch {i}\", flush=True)\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "        }, f'checkpoint_epoch_{epoch}_step_{i}.pth')\n",
    " \n",
    "        gleu_score = evaluate_gleu(model, val_dataloader, metric, tokenizer)\n",
    "        val_loss = evaluate_loss(model, val_dataloader)\n",
    "        print(f\"-----Gleu Score On Validation Data: {gleu_score}\", flush=True)\n",
    "        print(f\"-----Val Loss: {val_loss}\", flush=True)\n",
    "        with open(\"gleu_scores.txt\", \"a\") as file:\n",
    "            file.write(f\"Epoch: {epoch} | Batch: {i} | Gleu Score: {gleu_score} | Val loss: {val_loss}\\n\")\n",
    "\n",
    "        scheduler.step(gleu_score)\n",
    "        \n",
    "        \n",
    "  epoch_loss /= len(train_dataloader)\n",
    "  print(f\"-----Training Loss: {epoch_loss}\", flush=True)\n",
    "    \n",
    "  # save after each epoch\n",
    "  torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "    }, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "  val_loss = evaluate_loss(model, val_dataloader)\n",
    "  \n",
    "  print(f\"-----Validation Loss: {val_loss}\", flush=True)\n",
    "  print(\"=============================================\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(model, metric, ungrammatical_sen, target =None):\n",
    "    model.eval()\n",
    "    google_bleu = None\n",
    "    \n",
    "    inputs = tokenizer(ungrammatical_sen, truncation=True, max_length=1024, return_tensors = \"pt\")\n",
    "\n",
    "    inputs = {k : v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.module.generate(**inputs,\n",
    "                                    max_length=len(ungrammatical_sen) + 20, \n",
    "                                    num_beams=5, \n",
    "                                    do_sample=True,\n",
    "                                   repetition_penalty=2.6,\n",
    "                                     temperature= 0.01\n",
    "                                    )\n",
    "\n",
    "    sentence = tokenizer.decode(outputs[0], skip_special_tokens=True )\n",
    "    \n",
    "    # compute gleu score between target and pred\n",
    "    if target:\n",
    "        google_bleu = metric.compute(predictions=[sentence], references=[target])\n",
    "        \n",
    "    return sentence, google_bleu\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: fix grammar:My names is ali and i went at school yesterday\n",
      "Output: My name is ali and i went to school yesterday.\n",
      "Google Bleu: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"Bitcoin is for $7,094 this morning, which CoinDesk says.\",\n",
    "#                target = \"Bitcoin goes for $7,094 this morning, according to CoinDesk.\")\n",
    "\n",
    "inp = \"fix grammar:My names is ali and i went at school yesterday\"\n",
    "\n",
    "sentence, score = correct_grammar(model, \n",
    "                                  metric, \n",
    "                                  inp,\n",
    "                            target = \"My name is ali and i went to school yesterday\")\n",
    "\n",
    "print(f\"Input: {inp}\")\n",
    "print(f\"Output: {sentence}\")\n",
    "if score:\n",
    "    print(f\"Google Bleu: {score['google_bleu'] }\")\n",
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"The effect of widespread dud targets two face up attack position monsters on the field\",\n",
    "#                target = 'The effect of \"widespread dud\" targets two face up attack position monsters on the field.')\n",
    "               \n",
    "\n",
    "# correct_grammar(model, metric,\n",
    "#                 \"tax on sales of stores for non residents are set at 21% for 2014 and 20% in 2015 payable on sales tentatively earned from the difference of the property value some time of purchase (price differences according to working time) and theyear to which sale couples (sales costs), based on the approved annual on the base approved by law).\",\n",
    "#                target = \"Capital Gains tax on the sale of properties for non-residents is set at 21% for 2014 and 20% in 2015 payable on profits earned on the difference of the property value between the year of purchase (purchase price plus costs) and the year of sale (sales price minus costs), based on the approved annual percentage increase on the base value approved by law.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
